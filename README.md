# ğŸ§  llm-deploy-lab (Internal Only)

> âœ… Deploy LLM (e.g. Falcon-RW-1B) via FastAPI + Streamlit + Docker  
> ğŸ› ï¸ Used for internal experimentation â€” not intended for public use or production

## ğŸš§ Status
This project is considered **complete for internal purposes only**.  
The result is not production-quality and **should not be showcased**.

---

## ğŸ“¦ Stack

- **FastAPI** â€“ For serving LLM API at `/generate`
- **Streamlit** â€“ Simple frontend UI for user interaction
- **Docker Compose** â€“ To run LLM backend + UI in isolated containers
- **Transformers Pipeline** â€“ Uses `pipeline("text-generation")` to serve HuggingFace models

---

## ğŸ–¼ï¸ Architecture

```plaintext
[ Streamlit UI ]
        |
        v
[ FastAPI + Transformers pipeline ]
        |
        v
[ CPU-based LLM model (e.g. Falcon-RW-1B) ]

---

ğŸ§ª Example Prompt (Postman or CLI)

curl -X POST http://localhost:8000/generate \
  -H "Content-Type: application/json" \
  -d '{"prompt": "à¸ªà¸§à¸±à¸ªà¸”à¸µ"}'

Response:

{
  "result": "Hello .... (sample text)"
}

ğŸ§± Folder Structure
.
â”œâ”€â”€ app.py                 # FastAPI backend
â”œâ”€â”€ web_ui.py             # Streamlit UI
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml

âš ï¸ Limitation
âŒ Very slow (running on CPU, large model load time)

âŒ Output quality poor (Falcon-RW-1B is small, not tuned)

âŒ Not optimized for inference

âŒ Not suitable for demo/public exposure

âœ… âœ… BUT: Was successfully deployed, works end-to-end

ğŸªµ Internal Note
This project was used to practice real LLM deployment flow â€”
from building an API to UI, Dockerization, model loading, and prompt-response roundtrip.

Itâ€™s now archived and used as internal proof of work.